{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yangxuan8/CMI-EVSI/blob/main/CMI_EVSI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to Google Drive"
      ],
      "metadata": {
        "id": "o-jJ9YxJW8f-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPxj6yhcUZ4s",
        "outputId": "42100670-7591-43fb-8be3-ebc1927ec0da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standard imports"
      ],
      "metadata": {
        "id": "9d1RudA-W_dr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-ls2I-MchUQ",
        "outputId": "68951f12-3104-4eee-d0be-cdd207713f2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.2.1-py3-none-any.whl (801 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.10.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (2.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->pytorch_lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->pytorch_lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.10.1 pytorch_lightning-2.2.1 torchmetrics-1.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_lightning\n",
        "import torch\n",
        "import pickle\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchmetrics import AUROC, Accuracy\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset, ConcatDataset\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import pandas as pd\n",
        "sys.path.append('/content/drive/MyDrive/DIME_main/experiments/hev')\n",
        "import feature_groups\n",
        "sys.path.append('/content/drive/MyDrive/DIME_main')\n",
        "import dime\n",
        "from dime.data_utils import HEVDataset, get_group_matrix, get_xy\n",
        "from dime import MaskingPretrainer, CMIEstimator\n",
        "from dime.utils import StaticMaskLayer1d, ConcreteMask, get_confidence, MaskLayerGrouped, get_mlp_network\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from baseline_models.base_model import BaseModel\n",
        "sys.path.append('/content/drive/MyDrive/DIME_main/experiments')\n",
        "from baselines import eddi, pvae, dfs\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up command line arguments"
      ],
      "metadata": {
        "id": "yPMHQy_vYAb0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhfLpt3Qv2JA",
        "outputId": "16ff4159-2156-450f-86a7-88738e35a399"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--num_trials'], dest='num_trials', nargs=None, const=None, default=3, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--gpu', type=int, default=0)\n",
        "parser.add_argument('--method', type=str, default='dfs', choices=['eddi', 'dfs', 'apsa', 'fully_supervised'])\n",
        "parser.add_argument('--use_feature_costs', default=False, action=\"store_true\")\n",
        "parser.add_argument('--num_trials', type=int, default=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "diDnBpycYd4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Directly load the sampled dataset from csv without considering dataset splitting, for more details on obtaining the sample dataset, please refer to sample_dataset.ipynb"
      ],
      "metadata": {
        "id": "pA0MsspZO01E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hev_feature_names = feature_groups.hev_feature_names\n",
        "hev_feature_groups = feature_groups.hev_feature_groups\n",
        "auc_metric = AUROC(task='multiclass', num_classes=3)\n",
        "acc_metric = Accuracy(task='multiclass', num_classes=3)\n",
        "\n",
        "# Parse args\n",
        "args = parser.parse_known_args()[0]\n",
        "device = torch.device('cuda', args.gpu)\n",
        "num_trials = args.num_trials\n",
        "cols_to_drop = []\n",
        "if cols_to_drop is not None:\n",
        "    hev_feature_names = [item for item in hev_feature_names if str(hev_feature_names.index(item)) not in cols_to_drop]\n",
        "# Load dataset\n",
        "dataset = HEVDataset(data_dir=1, cols_to_drop=cols_to_drop)\n",
        "d_in = dataset.X.shape[1]  # 32\n",
        "d_out = len(np.unique(dataset.Y))  # 3\n",
        "\n",
        "# Get features and groups\n",
        "feature_groups_dict, feature_groups_mask = get_group_matrix(hev_feature_names, hev_feature_groups)\n",
        "feature_group_indices = {i : key for i, key in enumerate(feature_groups_dict.keys())}\n",
        "feat_to_ind = {key: i for i, key in enumerate(hev_feature_names)}\n",
        "\n",
        "num_groups = len(feature_groups_mask)  # 32\n",
        "print(\"Num groups=\", num_groups)\n",
        "print(\"Num features=\", d_in)\n",
        "\n",
        "# Split dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(0))\n",
        "daataset_dict = dict(train_dataset=train_dataset, val_dataset=val_dataset, test_dataset=test_dataset)\n",
        "f = open('/content/drive/MyDrive/dataset/data/dataset.pkl', \"wb\", pickle.HIGHEST_PROTOCOL)\n",
        "pickle.dump(daataset_dict, f)\n",
        "\n",
        "print(f'Train samples = {len(train_dataset)}, val samples = {len(val_dataset)}, test samples = {len(test_dataset)}')\n",
        "# Find mean/variance for normalizing\n",
        "x, y = get_xy(train_dataset)\n",
        "mean = np.mean(x, axis=0)\n",
        "std = np.clip(np.std(x, axis=0), 1e-3, None)\n",
        "\n",
        "# Normalize via the original dataset\n",
        "if args.method == 'eddi':\n",
        "    dataset.X = (dataset.X - mean)/std\n",
        "else:\n",
        "    dataset.X = dataset.X - mean\n",
        "\n",
        "# Set up data loaders.\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=128, shuffle=True, pin_memory=True,\n",
        "    drop_last=True, num_workers=4)\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset, batch_size=1024, shuffle=False, pin_memory=True, drop_last=True, num_workers=4)\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=1024, shuffle=False, pin_memory=True, drop_last=True, num_workers=4)\n",
        "\n",
        "\n",
        "mask_layer = MaskLayerGrouped(append=True, group_matrix=torch.tensor(feature_groups_mask))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbxpI_brlc0j",
        "outputId": "48e65fb9-a88b-4a73-c829-3ee2c53342b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['faultNumber', 'VelocityRef:1', '<xdot>', '<BattSoc>', '<BattPwr>',\n",
            "       '<Cltch1State>', '<Cltch2State>', '<BattV>', '<TransGear>', '<EngSpd>',\n",
            "       '<IntkVlvLift>', '<EngTrq>', '<ThrPosPct>', '<WgAreaPct>',\n",
            "       '<EgrVlvAreaPct>', '<VarCompRatioPos>', '<Acc>', '<Dec>', '<IgSw>',\n",
            "       '<Chrg>', 'TransGear', 'BrkCmd', 'Cltch1Cmd', '<MotTrq>', '<StartTrq>',\n",
            "       'StartCmd', 'MotTrqCmd', 'BattCrnt:1', 'MotPwrElec:1', 'MotPwrMech:1',\n",
            "       'IntkVlvLiftCmd', 'FuelMainSoi', 'FuelFlw'],\n",
            "      dtype='object')\n",
            "Num groups= 32\n",
            "Num features= 32\n",
            "Train samples = 32902, val samples = 3656, test samples = 3655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up networks"
      ],
      "metadata": {
        "id": "0k5q2bapZA6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original input is 2d_in, which is performed without considering the feature group. And right now we consider the Group features."
      ],
      "metadata": {
        "id": "d4PABEJcSVGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden = 128\n",
        "dropout = 0.3\n",
        "predictor = nn.Sequential(\n",
        "                      nn.Linear(d_in + num_groups, hidden),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(dropout),\n",
        "                      nn.Linear(hidden, hidden),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(dropout),\n",
        "                      nn.Linear(hidden, d_out))\n",
        "\n",
        "value_network = nn.Sequential(\n",
        "                      nn.Linear(d_in + num_groups, hidden),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(dropout),\n",
        "                      nn.Linear(hidden, hidden),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(dropout),\n",
        "                      nn.Linear(hidden, num_groups))\n",
        "\n",
        "gdfs = dfs.GreedyDynamicSelection(selector, predictor, mask_layer).to(device)"
      ],
      "metadata": {
        "id": "MyLYBFISZB1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Process with different methods"
      ],
      "metadata": {
        "id": "NQF6M2S4dpum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. EDDI: Efficient dynamic discovery of high-value information with partial VAE, a dynamic feature selection method that relies on a generative model\n",
        "2. DFS: The simplest dynamic feature selection method provided considering the simple neural network\n",
        "3. APSA: Our Access Point Search Algorithm, we consider non-uniform feature acquisition costs and combine the losses of the selector(value network) and predictor. For more details please refer to cmi_estimator.py\n",
        "4. fully_supervised: simplest and fastest method"
      ],
      "metadata": {
        "id": "bHHJuF_qVIEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = [1, 3, 5, 10, 15, 20, 25, 30]\n",
        "use_feature_costs = False\n",
        "feature_costs = None\n",
        "if args.use_feature_costs:\n",
        "    feature_cost_df = pd.read_csv(\"data/feature_list_hev.csv\")\n",
        "    feature_costs = [feature_cost_df[feature_cost_df['Feature Name'] == feature]['Cost (Hours)'].item() for feature in list(feature_groups_dict.keys())]\n",
        "    use_feature_costs = True\n",
        "\n",
        "for trial in range(num_trials):\n",
        "\n",
        "    results_dict = {\n",
        "        'acc': {},\n",
        "        'features': {}\n",
        "    }\n",
        "\n",
        "\n",
        "    if args.method == 'eddi':\n",
        "        # Train PVAE.\n",
        "        bottleneck = 16\n",
        "        hidden = 128\n",
        "        dropout = 0.3\n",
        "        encoder = get_mlp_network(d_in + num_groups, bottleneck * 2)\n",
        "        decoder = get_mlp_network(bottleneck, d_in)\n",
        "\n",
        "        pv = pvae.PVAE(encoder, decoder, mask_layer, 20, 'gaussian').to(device)\n",
        "        pv.fit(\n",
        "            train_dataloader,\n",
        "            val_dataloader,\n",
        "            lr=1e-3,\n",
        "            nepochs=10,\n",
        "            verbose=False)\n",
        "\n",
        "        # Train masked predictor.\n",
        "        model = get_mlp_network(d_in + num_groups, d_out)\n",
        "        sampler = None\n",
        "        # if trial == 0:\n",
        "        sampler = iterative.UniformSampler(get_xy(train_dataset)[0])  # TODO don't actually need sampler\n",
        "        iterative_selector = iterative.IterativeSelector(model, mask_layer, sampler).to(device)\n",
        "        iterative_selector.fit(\n",
        "            train_dataloader,\n",
        "            val_dataloader,\n",
        "            lr=1e-3,\n",
        "            nepochs=10,\n",
        "            loss_fn=nn.CrossEntropyLoss(),\n",
        "            patience=5,\n",
        "            verbose=True)\n",
        "\n",
        "        # Set up EDDI feature selection object.\n",
        "        eddi_selector = eddi.EDDI(pv, model, mask_layer, feature_costs=feature_costs).to(device)\n",
        "\n",
        "        # Evaluate.\n",
        "        metrics_dict, cost_dict = eddi_selector.evaluate_multiple(test_dataloader, num_features, auc_metric, verbose=False)\n",
        "        for num in num_features:\n",
        "            acc = metrics_dict[num]\n",
        "            results_dict['acc'][num] = acc\n",
        "            print(f'Num = {num}, Acc = {100*acc:.2f}')\n",
        "\n",
        "        print(results_dict)\n",
        "        print(cost_dict)\n",
        "        with open(f'/content/drive/MyDrive/dataset/results/hev_{args.method}_trial_{trial}_feature_costs_{use_feature_costs}.pkl', 'wb') as f:\n",
        "            pickle.dump(results_dict, f)\n",
        "\n",
        "        with open(f'/content/drive/MyDrive/dataset/results/hev_costs_{args.method}_trial_{trial}_feature_costs_{use_feature_costs}.pkl', 'wb') as f:\n",
        "            pickle.dump(cost_dict, f)\n",
        "\n",
        "    if args.method == 'dfs':\n",
        "        max_features = 32\n",
        "\n",
        "        # Prepare networks.\n",
        "        predictor = get_mlp_network(d_in + num_groups, d_out)\n",
        "        selector = get_mlp_network(d_in + num_groups, num_groups)\n",
        "\n",
        "        # Pretrain predictor\n",
        "\n",
        "        pretrain = MaskingPretrainer(\n",
        "            predictor,\n",
        "            mask_layer,\n",
        "            lr=1e-3,\n",
        "            loss_fn=nn.CrossEntropyLoss(),\n",
        "            val_loss_fn=auc_metric)\n",
        "\n",
        "        trainer = pl.Trainer(max_epochs=10)\n",
        "        trainer.fit(pretrain, train_dataloader, val_dataloader)\n",
        "\n",
        "        # Train selector and predictor jointly.\n",
        "        gdfs = dfs.GreedyDynamicSelection(selector, predictor, mask_layer).to(device)\n",
        "        gdfs.fit(\n",
        "            train_dataloader,\n",
        "            val_dataloader,\n",
        "            lr=1e-3,\n",
        "            nepochs=10,\n",
        "            max_features=max_features,\n",
        "            loss_fn=nn.CrossEntropyLoss(),\n",
        "            patience=3,\n",
        "            verbose=True)\n",
        "\n",
        "        # Evaluate.\n",
        "        for num in num_features:\n",
        "            auroc_list = []\n",
        "            acc_list = []\n",
        "\n",
        "            auroc, acc = gdfs.evaluate(test_dataloader, num, (auc_metric, acc_metric))\n",
        "            #results_dict['acc'][num] = acc\n",
        "            #print(f'Num = {num}, Acc = {100*acc:.2f}')\n",
        "            auroc_list.append(auroc)\n",
        "            acc_list.append(acc)\n",
        "            print(f'Num = {num}, AUROC = {100*auroc:.2f}, Acc = {100*acc:.2f}')\n",
        "\n",
        "        with open(f'/content/drive/MyDrive/dataset/results/hev_{args.method}_trial_{trial}.pkl', 'wb') as f:\n",
        "            pickle.dump(results_dict, f)\n",
        "\n",
        "        # Save model\n",
        "        gdfs.cpu()\n",
        "        torch.save(gdfs, f'/content/drive/MyDrive/dataset/results/hev_{args.method}_trial_{trial}.pt')\n",
        "\n",
        "    if args.method == 'apsa':\n",
        "        max_features = 32\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Prepare networks.\n",
        "        hidden = 128\n",
        "        dropout = 0.3\n",
        "        predictor = nn.Sequential(\n",
        "                  nn.Linear(d_in + num_groups, hidden),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Dropout(dropout),\n",
        "                  nn.Linear(hidden, hidden),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Dropout(dropout),\n",
        "                  nn.Linear(hidden, d_out))\n",
        "\n",
        "        value_network = nn.Sequential(\n",
        "                  nn.Linear(d_in + num_groups, hidden),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Dropout(dropout),\n",
        "                  nn.Linear(hidden, hidden),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Dropout(dropout),\n",
        "                  nn.Linear(hidden, num_groups))\n",
        "\n",
        "        # Pretrain predictor\n",
        "        pretrain = MaskingPretrainer(\n",
        "            predictor,\n",
        "            mask_layer,\n",
        "            lr=1e-3,\n",
        "            patience=6,\n",
        "            loss_fn=nn.CrossEntropyLoss(),\n",
        "            val_loss_fn=auc_metric\n",
        "            )\n",
        "\n",
        "        trainer = pl.Trainer(max_epochs=30)\n",
        "        trainer.fit(pretrain, train_dataloader, val_dataloader)\n",
        "\n",
        "        apsa = CMIEstimator(value_network,\n",
        "                  predictor,\n",
        "                  mask_layer,\n",
        "                  lr=1e-3,\n",
        "                  min_lr=1e-6,\n",
        "                  max_features=30,\n",
        "                  eps=0.1,\n",
        "                  loss_fn=nn.CrossEntropyLoss(reduction='none'),\n",
        "                  val_loss_fn=auc_metric,\n",
        "                  eps_decay=0.2,\n",
        "                  eps_steps=10,\n",
        "                  patience=5,\n",
        "                  feature_costs=feature_costs,\n",
        "                  cmi_scaling='bounded')\n",
        "\n",
        "        trainer = Trainer(accelerator='gpu',\n",
        "                      devices=[args.gpu],\n",
        "                      max_epochs=30,\n",
        "                      precision=16,\n",
        "                      logger=logger,\n",
        "                      num_sanity_val_steps=0,\n",
        "                      callbacks=[checkpoint_callback],\n",
        "                      log_every_n_steps=10)\n",
        "\n",
        "        trainer.fit(greedy_cmi_estimator, train_dataloader, val_dataloader)\n",
        "\n",
        "        # Evaluate.\n",
        "        for num in num_features:\n",
        "            auroc_list = []\n",
        "            acc_list = []\n",
        "            auroc, acc = apsa.evaluate(test_dataloader, num, (auc_metric, acc_metric))\n",
        "            auroc_list.append(auroc)\n",
        "            acc_list.append(acc)\n",
        "            print(f'Num = {num}, AUROC = {100*auroc:.2f}, Acc = {100*acc:.2f}')\n",
        "\n",
        "\n",
        "        with open(f'/content/drive/MyDrive/dataset/results/hev_{args.method}_trial_{trial}.pkl', 'wb') as f:\n",
        "            pickle.dump(results_dict, f)\n",
        "\n",
        "        # Save model\n",
        "        gdfs.cpu()\n",
        "        torch.save(gdfs, f'/content/drive/MyDrive/dataset/results/hev_{args.method}_trial_{trial}.pt')\n",
        "\n",
        "    # Train with full input\n",
        "    if args.method == 'fully_supervised':\n",
        "        model = get_mlp_network(d_in, d_out).to(device)\n",
        "        opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                opt, mode='min', factor=0.2, patience=5,\n",
        "                min_lr=1e-5, verbose=True)\n",
        "\n",
        "        num_bad_epochs = 0\n",
        "        early_stopping_epochs = 6\n",
        "\n",
        "        for epoch in range(20):\n",
        "            model.train()\n",
        "            train_batch_loss = 0\n",
        "            val_batch_loss = 0\n",
        "            val_pred_list = []\n",
        "            val_y_list = []\n",
        "\n",
        "            for i, (x, y) in enumerate(tqdm(train_dataloader)):\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "\n",
        "                pred = model(x)\n",
        "                train_loss = criterion(pred, y)\n",
        "                train_batch_loss += train_loss.item()\n",
        "                train_loss.backward()\n",
        "                opt.step()\n",
        "                model.zero_grad()\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for i, (x, y) in enumerate(tqdm(val_dataloader)):\n",
        "                    x = x.to(device)\n",
        "                    y = y.to(device)\n",
        "\n",
        "                    pred = model(x)\n",
        "                    val_loss = criterion(pred, y)\n",
        "                    val_batch_loss += val_loss.item()\n",
        "                    val_pred_list.append(pred.cpu())\n",
        "                    val_y_list.append(y.cpu())\n",
        "\n",
        "                scheduler.step(val_batch_loss/len(val_dataloader))\n",
        "                val_loss = val_batch_loss/len(val_dataloader)\n",
        "                # Check if best model.\n",
        "                if val_loss == scheduler.best:\n",
        "                    # best_model = deepcopy(model)\n",
        "                    num_bad_epochs = 0\n",
        "                else:\n",
        "                    num_bad_epochs += 1\n",
        "\n",
        "                # Early stopping.\n",
        "                if num_bad_epochs > early_stopping_epochs:\n",
        "                    print(f'Stopping early at epoch {epoch+1}')\n",
        "                    break\n",
        "\n",
        "            print(f\"Epoch: {epoch}, Train Loss: {train_batch_loss/len(train_dataloader)},\"\n",
        "                  + f\"Val Loss: {val_batch_loss/len(val_dataloader)},\"\n",
        "                  + f\"Val Performance: {auc_metric(torch.cat(val_pred_list), torch.cat(val_y_list))}\")\n",
        "\n",
        "        print(\"Evaluating on test set\")\n",
        "\n",
        "        model.eval()\n",
        "        confidence_list = []\n",
        "        test_pred_list = []\n",
        "        test_y_list = []\n",
        "        for i, (x, y) in enumerate(tqdm(test_dataloader)):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            pred = model(x)\n",
        "            test_pred_list.append(pred.cpu())\n",
        "            test_y_list.append(y.cpu())\n",
        "\n",
        "            confidence_list.append(get_confidence(pred.cpu()))\n",
        "\n",
        "        print(f\"Test Performance:{auc_metric(torch.cat(test_pred_list), torch.cat(test_y_list))}\")\n",
        "        with open('/content/drive/MyDrive/dataset/confidence.npy', 'wb') as f:\n",
        "            np.save(f, np.array(torch.cat(confidence_list).detach().numpy()))\n"
      ],
      "metadata": {
        "id": "YO0lbOXfaBD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPtYUG7ME-jK",
        "outputId": "a0b42cb2-1dad-42e7-b9db-1a4f0e12fd2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "gdfs.selector.load_state_dict(torch.load('/content/drive/MyDrive/selPre/selector1'))\n",
        "gdfs.predictor.load_state_dict(torch.load('/content/drive/MyDrive/selPre/predictor1'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate performance"
      ],
      "metadata": {
        "id": "Ne4p0HU2fI8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance on the test dataset"
      ],
      "metadata": {
        "id": "JIVUdAYSZuJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For saving results.\n",
        "num_features = [1, 3, 5, 10, 15, 20, 25, 30]\n",
        "auroc_list = []\n",
        "acc_list = []\n",
        "\n",
        "# Metrics (softmax is applied automatically in recent versions of torchmetrics).\n",
        "auroc_metric = lambda pred, y: AUROC(task='multiclass', num_classes=3)(pred.softmax(dim=1), y)\n",
        "acc_metric = Accuracy(task='multiclass', num_classes=3)\n",
        "\n",
        "# Evaluate.\n",
        "for num in num_features:\n",
        "    auroc, acc = apsa.evaluate(test_dataloader, num, (auroc_metric, acc_metric))\n",
        "    auroc_list.append(auroc)\n",
        "    acc_list.append(acc)\n",
        "    print(f'Num = {num}, AUROC = {100*auroc:.2f}, Acc = {100*acc:.2f}')"
      ],
      "metadata": {
        "id": "iE-bonLQZyPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot results"
      ],
      "metadata": {
        "id": "QLgxTdXIfaCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axarr = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "# AUROC\n",
        "plt.sca(axarr[0])\n",
        "plt.plot(num_features, auroc_list, marker='o')\n",
        "plt.xlabel('# Features', fontsize=16)\n",
        "plt.ylabel('AUROC', fontsize=16)\n",
        "plt.title('AUROC vs. Feature Budget', fontsize=18)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "\n",
        "# Accuracy\n",
        "plt.sca(axarr[1])\n",
        "plt.plot(num_features, acc_list, marker='o')\n",
        "plt.xlabel('# Features', fontsize=16)\n",
        "plt.ylabel('Accuracy', fontsize=16)\n",
        "plt.title('Accuracy vs. Feature Budget', fontsize=18)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MJC3MNK9fX_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVSI Functions"
      ],
      "metadata": {
        "id": "lljrmZm7fgE3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdQRhj2SKllC"
      },
      "outputs": [],
      "source": [
        "cost_false_positive = 2\n",
        "cost_false_negative = 4\n",
        "\n",
        "x , y_train_tensor = get_xy(train_dataset)\n",
        "#x_np=x.numpy()\n",
        "x_train_df=pd.DataFrame.from_records(x)\n",
        "y_train_tensor = torch.tensor(y_train_tensor)\n",
        "unique_elements, counts = torch.unique(y_train_tensor, return_counts=True)\n",
        "for i in range(unique_elements.size(0)):\n",
        "  if unique_elements[i]==0:\n",
        "    non_fault = counts[i]\n",
        "    break\n",
        "total = torch.sum(counts)\n",
        "\n",
        "x , y_val_tensor = get_xy(val_dataset)\n",
        "#x_np=x.numpy()\n",
        "y_val_tensor = torch.tensor(y_val_tensor)\n",
        "x_cv_df=pd.DataFrame.from_records(x)\n",
        "unique_elements, counts = torch.unique(y_val_tensor, return_counts=True)\n",
        "for i in range(unique_elements.size(0)):\n",
        "  if unique_elements[i]==0:\n",
        "    non_fault += counts[i]\n",
        "total += torch.sum(counts)\n",
        "\n",
        "P_present = non_fault/total\n",
        "P_absent = 1 - P_present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZSIWbCGL40t"
      },
      "outputs": [],
      "source": [
        "# helper function to calculate probability of correctly giving signal when present\n",
        "def get_signal_present(prediction, ground_truth):\n",
        "    present_index = list()\n",
        "    for i in range(len(ground_truth)):\n",
        "        if ground_truth[i] == 0:\n",
        "            present_index.append(i)\n",
        "\n",
        "    counter = 0\n",
        "    for index in present_index:\n",
        "        if prediction[index] == 0:\n",
        "            counter += 1\n",
        "\n",
        "    return counter/len(present_index)\n",
        "\n",
        "# helper function to calculate probability of correctly giving signal when present\n",
        "# there should be a more generic way using operator module to merge this with the one above.\n",
        "def get_no_signal_absent(prediction, ground_truth):\n",
        "    absent_index = list()\n",
        "    for i in range(len(ground_truth)):\n",
        "        if ground_truth[i] != 0:\n",
        "            absent_index.append(i)\n",
        "\n",
        "    counter = 0\n",
        "    for index in absent_index:\n",
        "        if prediction[index] != 0:\n",
        "            counter += 1\n",
        "    return counter/len(absent_index)\n",
        "\n",
        "def get_expected_cost(prediction,GT_tensor):\n",
        "  # get P(signal|present) and P(no signal|absent)\n",
        "    ground_truth = GT_tensor.tolist()\n",
        "    #prediction = prediction_t.tolist()\n",
        "    P_signal_present = get_signal_present(prediction, ground_truth)\n",
        "    P_no_signal_absent = get_no_signal_absent(prediction, ground_truth)\n",
        "    P_signal_absent = 1 - P_no_signal_absent\n",
        "    P_no_signal_present = 1 - P_signal_present\n",
        "\n",
        "  # get P(signal)\n",
        "    P_signal = P_present * P_signal_present + P_absent * P_signal_absent\n",
        "    P_no_signal = 1 - P_signal\n",
        "\n",
        "  # bayesian probability\n",
        "    P_absent_signal = (P_signal_absent * P_absent) / P_signal\n",
        "    P_present_signal = (P_signal_present * P_present) / P_signal\n",
        "    P_absent_no_signal = (P_no_signal_absent * P_absent) / P_no_signal\n",
        "    P_present_no_signal = (P_no_signal_present * P_present) / P_no_signal\n",
        "\n",
        "  #calculate the evoi\n",
        "    evoi = P_signal * min(cost_false_positive * P_absent_signal, cost_false_negative * P_present_signal) + P_no_signal * min(cost_false_positive * P_absent_no_signal, cost_false_negative * P_present_no_signal)\n",
        "\n",
        "    return evoi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot selections"
      ],
      "metadata": {
        "id": "ZTY8K1tjflru"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBbJwwJuMa91",
        "outputId": "a0ab295f-0114-4d60-e04b-83a86ec00fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "\n",
            "['<BattSoc>']\n",
            "tensor(1.1450)\n",
            "2\n",
            "\n",
            "['<BattSoc>', '<ThrPosPct>']\n",
            "tensor(0.9068)\n",
            "3\n",
            "\n",
            "['<BattSoc>', '<BattV>', '<ThrPosPct>']\n",
            "tensor(0.9322)\n",
            "4\n",
            "\n",
            "['<BattSoc>', '<BattV>', '<ThrPosPct>', '<WgAreaPct>']\n",
            "tensor(0.8946)\n",
            "5\n",
            "\n",
            "['<BattSoc>', '<BattV>', '<ThrPosPct>', '<WgAreaPct>', '<IntkVlvLift>']\n",
            "tensor(0.8813)\n",
            "6\n",
            "\n",
            "['<BattSoc>', '<BattV>', '<ThrPosPct>', '<WgAreaPct>', '<IntkVlvLift>', '<Cltch2State>']\n",
            "tensor(0.9003)\n",
            "7\n",
            "\n",
            "['<BattSoc>', '<BattV>', '<IntkVlvLift>', '<ThrPosPct>', '<WgAreaPct>', '<EngSpd>', '<Cltch2State>']\n",
            "tensor(0.9104)\n",
            "8\n",
            "\n",
            "['<Cltch2State>', '<WgAreaPct>', '<BattSoc>', '<ThrPosPct>', '<IntkVlvLift>', '<BattV>', '<EngSpd>', 'BattCrnt:1']\n",
            "tensor(0.7839)\n",
            "9\n",
            "\n",
            "['<Cltch2State>', '<EngSpd>', '<WgAreaPct>', '<BattSoc>', '<ThrPosPct>', 'BattCrnt:1', '<IntkVlvLift>', '<BattV>', 'MotTrqCmd']\n",
            "tensor(0.6943)\n",
            "10\n",
            "\n",
            "['<ThrPosPct>', '<EngSpd>', '<BattSoc>', 'BattCrnt:1', 'MotTrqCmd', '<IntkVlvLift>', '<WgAreaPct>', '<Cltch2State>', '<BattV>', 'IntkVlvLiftCmd']\n",
            "tensor(0.6732)\n",
            "11\n",
            "\n",
            "['<WgAreaPct>', '<BattV>', '<IntkVlvLift>', 'BattCrnt:1', 'MotTrqCmd', '<BattSoc>', '<EngSpd>', '<ThrPosPct>', '<Cltch2State>', 'IntkVlvLiftCmd', 'MotPwrMech:1']\n",
            "tensor(0.7819)\n",
            "12\n",
            "\n",
            "['<WgAreaPct>', '<BattV>', 'BattCrnt:1', 'MotTrqCmd', '<BattSoc>', '<IntkVlvLift>', '<EngSpd>', '<ThrPosPct>', '<Cltch2State>', 'IntkVlvLiftCmd', '<EngTrq>', 'MotPwrMech:1']\n",
            "tensor(0.7471)\n",
            "13\n",
            "\n",
            "['<BattSoc>', 'BattCrnt:1', 'MotTrqCmd', '<ThrPosPct>', '<Cltch2State>', '<WgAreaPct>', '<BattV>', '<EngSpd>', '<IntkVlvLift>', 'IntkVlvLiftCmd', '<EngTrq>', 'MotPwrMech:1', '<TransGear>']\n",
            "tensor(0.7334)\n",
            "14\n",
            "\n",
            "['<ThrPosPct>', '<EngSpd>', '<BattSoc>', 'BattCrnt:1', 'MotTrqCmd', '<Cltch2State>', '<WgAreaPct>', '<IntkVlvLift>', '<BattV>', 'IntkVlvLiftCmd', 'MotPwrMech:1', '<EngTrq>', '<TransGear>', 'TransGear']\n",
            "tensor(0.7599)\n",
            "15\n",
            "\n",
            "['<ThrPosPct>', '<EngSpd>', '<BattSoc>', 'BattCrnt:1', 'MotTrqCmd', '<Cltch2State>', '<WgAreaPct>', '<IntkVlvLift>', '<BattV>', 'IntkVlvLiftCmd', 'MotPwrMech:1', '<EngTrq>', '<TransGear>', 'TransGear', 'VelocityRef:1']\n",
            "tensor(0.7488)\n",
            "16\n",
            "\n",
            "['<ThrPosPct>', '<EngSpd>', '<BattSoc>', 'BattCrnt:1', 'MotTrqCmd', '<Cltch2State>', '<WgAreaPct>', '<IntkVlvLift>', '<BattV>', '<TransGear>', 'IntkVlvLiftCmd', 'MotPwrMech:1', '<EngTrq>', 'TransGear', 'VelocityRef:1', '<VarCompRatioPos>']\n",
            "tensor(0.7326)\n",
            "17\n",
            "\n",
            "['<BattV>', '<EngSpd>', '<BattSoc>', 'BattCrnt:1', 'MotTrqCmd', '<ThrPosPct>', '<Cltch2State>', '<WgAreaPct>', '<IntkVlvLift>', '<TransGear>', 'IntkVlvLiftCmd', 'MotPwrMech:1', 'VelocityRef:1', '<EngTrq>', 'TransGear', '<VarCompRatioPos>', 'StartCmd']\n",
            "tensor(0.7246)\n",
            "18\n",
            "\n",
            "['<ThrPosPct>', '<BattV>', '<IntkVlvLift>', '<EngSpd>', '<BattSoc>', 'BattCrnt:1', 'MotTrqCmd', '<TransGear>', '<Cltch2State>', '<WgAreaPct>', 'VelocityRef:1', 'IntkVlvLiftCmd', 'MotPwrMech:1', '<EngTrq>', 'TransGear', '<VarCompRatioPos>', '<MotTrq>', 'StartCmd']\n",
            "tensor(0.7100)\n",
            "19\n",
            "\n",
            "['<WgAreaPct>', '<BattSoc>', 'BattCrnt:1', 'MotTrqCmd', '<Cltch2State>', '<BattV>', '<TransGear>', '<EngSpd>', '<IntkVlvLift>', '<ThrPosPct>', 'VelocityRef:1', 'MotPwrMech:1', 'IntkVlvLiftCmd', '<EngTrq>', 'TransGear', '<MotTrq>', 'FuelFlw', '<VarCompRatioPos>', 'StartCmd']\n",
            "tensor(0.7161)\n",
            "20\n",
            "\n",
            "['VelocityRef:1', '<Cltch2State>', '<IntkVlvLift>', '<EngSpd>', '<TransGear>', '<WgAreaPct>', '<BattV>', 'MotTrqCmd', 'BattCrnt:1', '<BattSoc>', '<ThrPosPct>', 'MotPwrMech:1', 'IntkVlvLiftCmd', '<EngTrq>', 'TransGear', 'Cltch1Cmd', '<MotTrq>', 'FuelFlw', '<VarCompRatioPos>', '<IgSw>']\n",
            "tensor(0.6857)\n",
            "21\n",
            "\n",
            "['VelocityRef:1', '<Cltch2State>', '<IntkVlvLift>', '<EngSpd>', '<TransGear>', '<BattV>', '<WgAreaPct>', '<ThrPosPct>', '<BattSoc>', 'MotTrqCmd', 'BattCrnt:1', '<MotTrq>', 'MotPwrMech:1', 'IntkVlvLiftCmd', '<EngTrq>', 'Cltch1Cmd', 'TransGear', '<xdot>', 'FuelFlw', '<IgSw>', '<Dec>']\n",
            "tensor(0.6894)\n",
            "22\n",
            "\n",
            "['VelocityRef:1', '<Cltch2State>', '<IntkVlvLift>', '<EngSpd>', '<TransGear>', '<MotTrq>', '<WgAreaPct>', '<BattV>', 'MotTrqCmd', 'BattCrnt:1', '<BattSoc>', '<ThrPosPct>', 'TransGear', 'MotPwrMech:1', 'Cltch1Cmd', 'IntkVlvLiftCmd', '<EngTrq>', 'FuelFlw', '<IgSw>', '<xdot>', '<Dec>', '<VarCompRatioPos>']\n",
            "tensor(0.6975)\n",
            "23\n",
            "\n",
            "['FuelFlw', 'TransGear', '<BattSoc>', '<Cltch2State>', '<BattV>', '<TransGear>', '<EngSpd>', '<IntkVlvLift>', '<ThrPosPct>', '<WgAreaPct>', 'VelocityRef:1', 'IntkVlvLiftCmd', '<MotTrq>', 'MotTrqCmd', 'BattCrnt:1', 'Cltch1Cmd', 'MotPwrMech:1', '<EngTrq>', '<IgSw>', '<xdot>', 'StartCmd', '<Dec>', '<VarCompRatioPos>']\n",
            "tensor(0.6936)\n",
            "24\n",
            "\n",
            "['FuelFlw', 'TransGear', '<BattSoc>', '<Cltch2State>', '<BattV>', '<TransGear>', '<EngSpd>', '<IntkVlvLift>', '<ThrPosPct>', '<WgAreaPct>', '<Dec>', 'VelocityRef:1', 'MotPwrMech:1', 'Cltch1Cmd', '<MotTrq>', 'IntkVlvLiftCmd', 'MotTrqCmd', 'BattCrnt:1', '<IgSw>', '<VarCompRatioPos>', '<EngTrq>', '<xdot>', 'StartCmd', 'FuelMainSoi']\n",
            "tensor(0.6907)\n"
          ]
        }
      ],
      "source": [
        "# Generate selections for entire test set.\n",
        "num_features = range(1, 25)\n",
        "p_list = []\n",
        "\n",
        "for num in num_features:\n",
        "  print(str(num)+'\\n')\n",
        "  x, y = get_xy(test_dataset)\n",
        "  x = torch.tensor(x)\n",
        "  x_np=x.numpy()\n",
        "  x_test_df=pd.DataFrame(x)\n",
        "  pred1, x_masked, m = gdfs(x.cuda(), max_features=num)\n",
        "  pred=(pred1.cpu()).detach().numpy()\n",
        "  pred = np.argmax(pred, axis=-1)\n",
        "  p = m.mean(dim=0)\n",
        "  p_list.append(p.cpu().numpy())\n",
        "\n",
        "  sensors_index=np.array(p_list)\n",
        "  sorted_indices = np.flip(np.argsort(sensors_index))\n",
        "  sensors=[]\n",
        "  for i in range(0,num):\n",
        "    sensors = np.append(sensors, dataset.features[sorted_indices[0][i]])\n",
        "  sensors=sensors.tolist()\n",
        "  print(sensors)\n",
        "  base_indices=sorted_indices[0][num:].tolist()\n",
        "  sorted_indices=sorted_indices[0][0:num].tolist()\n",
        "  print(get_expected_cost(pred,y))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO55/N8hFN6kSj0+5WzBn6F",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}